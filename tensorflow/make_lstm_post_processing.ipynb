{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo post processing annotation clean-up\n",
    "\n",
    "The purpose of this notebook is to develop the model that takes yolo outputed bounding box predictions, concatenates them into sequences of consecutive frames, and then runs sequences through an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "from math import floor\n",
    "\n",
    "from lxml import etree\n",
    "import re\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the chunk below in order to import that developped classes that include the data generator and then RNN model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_repo = '/Users/guillaumekugener/Documents/USC/USC_docs/ml/surgical-training-project/'\n",
    "\n",
    "import sys\n",
    "import importlib  \n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, os.path.join(path_to_repo, 'tools'))\n",
    "\n",
    "from utils import bb_intersection_over_union, compare_detected_to_annotation, plot_single_frame_with_outlines, gt_as_pd_df\n",
    "from post_process_yolo_generator import PostProcessYoloGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and data\n",
    "\n",
    "The chunk below holds the information as to where our main dataset, images, and annotations directories are. We also load the classes file which maps the object classes to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_BASE_DIR = '/Users/guillaumekugener/Documents/USC/USC_docs/ml/datasets/'\n",
    "\n",
    "# Specific dataset\n",
    "DATASET_PATH = os.path.join(DATASET_BASE_DIR, 'clean-surgical-ds')\n",
    "IMAGES_DIR = os.path.join(DATASET_PATH, 'JPEGImages')\n",
    "ANNOTATION_DIR = os.path.join(DATASET_PATH, 'Annotations')\n",
    "\n",
    "# Where we will save our output files and stats\n",
    "OUTPUTS_FROM_DETECTION_DIR = os.path.join(DATASET_BASE_DIR, 'surgical-auto-labelled-videos')\n",
    "\n",
    "classes_file = os.path.join(DATASET_PATH, 'classes.name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_as_df = gt_as_pd_df(os.path.join(DATASET_BASE_DIR, 'cvat_output', 'Annotations'), 720)\n",
    "gt_as_df.to_csv(\n",
    "    os.path.join(DATASET_BASE_DIR, 'cvat_output', 'gt_labels.csv'),\n",
    "    sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_map = pd.read_csv(classes_file, sep='\\t',header=None)\n",
    "class_to_index = {}\n",
    "reverse_index_to_class = []\n",
    "for i, cn in enumerate(classes_map[0]):\n",
    "    class_to_index[cn] = int(i)\n",
    "    reverse_index_to_class.append(cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the chunk below, we keep a dictionary that defines the start and end frames of each of the videos that we are going to process.\n",
    "\n",
    "TODO: do this programatically (based on the frames in our dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_video_data_start_id = {\n",
    "    'S306T1': 1730,\n",
    "    'S306T2': 15140,\n",
    "    'S611T1': 23990, # This is the one used for validation\n",
    "    'S609T2': 20330\n",
    "}\n",
    "\n",
    "dict_video_data_end_id = {\n",
    "    'S306T1': 11100,\n",
    "    'S306T2': 21060,\n",
    "    'S611T1': 28800, # This is the one used for validation\n",
    "    'S609T2': 22890\n",
    "}\n",
    "\n",
    "training_videos = ['S306T1']#'S306T2', 'S609T2']\n",
    "validation_videos = ['S611T1']\n",
    "\n",
    "def make_frame_names(video_ids):\n",
    "    all_frames = []\n",
    "    for k in video_ids:\n",
    "        for t in classes_map.index.values:\n",
    "            all_frames += [k + '_frame_' + str(i).zfill(8) + '_tool_' + str(t) for i in range(dict_video_data_start_id[k], dict_video_data_end_id[k])]\n",
    "    return all_frames\n",
    "\n",
    "training_frames = make_frame_names(training_videos)\n",
    "validation_frames = make_frame_names(validation_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training frames: {len(training_frames)}\")\n",
    "print(f\"Number of validation frameS: {len(validation_frames)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training\n",
    "\n",
    "In the chunk below, we use our custom data generator function to generate our training and validation data. We also specify the training parameters (batch size, sequence length, number of objects we are predicting, shuffling, etc)\n",
    "\n",
    "TODO: the generator is quite slow in its current implementation (the focus was getting it to work in the first place). Would potentially be worthwhile to revisit its implementation to see if certain parts could be sped up. For example, I make use of for loops all over the place. There may be ways to take advantage of vectorized operations via numpy that could dramatically reduce compute time in data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'batch_size': 64, \n",
    "    'seq_len': 6, \n",
    "    'grid_size': 32, \n",
    "    'n_objects': 2, \n",
    "    'out_n_objects': 5, \n",
    "    'shuffle': False\n",
    "}\n",
    "training_generator = PostProcessYoloGenerator(\n",
    "    video_ids=dict_video_data_start_id, \n",
    "    video_data_dir=OUTPUTS_FROM_DETECTION_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    frame_ids=training_frames,\n",
    "    classes_map=class_to_index,\n",
    "    **params)\n",
    "\n",
    "validation_generator = PostProcessYoloGenerator(\n",
    "    video_ids=dict_video_data_start_id, \n",
    "    video_data_dir=OUTPUTS_FROM_DETECTION_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    frame_ids=validation_frames,\n",
    "    classes_map=class_to_index,\n",
    "    **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "The chunk below loads our model class and sets up the final components of our model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sequential_model import SequentialPostProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss actually needs to be two part... (since once is classification and the other one is regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SequentialPostProcess.build(\n",
    "    num_seq =  training_generator.dim[0], \n",
    "    num_features = training_generator.dim[1], \n",
    "    output_shape = training_generator.out_dim * 5)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is where we actually perform our model training\n",
    "\n",
    "We might want to rewrite this document as a training script. I am worried that if we move this to the cloud, we may lose connection and then training would fail. Having a script, we would be able to start a screen session and then not have to worry about dropping the ssh connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x=training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "In the sections below, we perform our model evaluation. We run the prediction on our training data and save the outputs to a pandas dataframe, along with the yolo outputs and ground truth outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not shuffled so the frames are in order\n",
    "prediction_params = {\n",
    "    'batch_size': 64, \n",
    "    'seq_len': 6, \n",
    "    'grid_size': 32, \n",
    "    'n_objects': 2, \n",
    "    'out_n_objects': 5, \n",
    "    'shuffle': False\n",
    "}\n",
    "\n",
    "frames_predicting_one = training_frames[:64*10]\n",
    "\n",
    "prediction_training_generator = PostProcessYoloGenerator(\n",
    "    video_ids=dict_video_data_start_id, \n",
    "    video_data_dir=OUTPUTS_FROM_DETECTION_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    frame_ids=frames_predicting_one,\n",
    "    classes_map=class_to_index,\n",
    "    **prediction_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "training_predictions = model.predict(\n",
    "    prediction_training_generator,\n",
    "    use_multiprocessing=True,\n",
    "    workers=6\n",
    ")\n",
    "print(f\"Time for prediciton: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three chunks below, we create three data frames (yolo, lstm, gt) corresponding to the detected objects in each of these datasets. We will then combine this into a single data frame that we will then use in order to do our final analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now save the outputs (video_id, tool, (score, bounding box)*n)\n",
    "prediction_output_data = {}\n",
    "pred_score_threshold = 0.15 # Ignore objects below this score to keep the output small\n",
    "bb_col_names = ['score', 'x1', 'y1', 'x2', 'y2']\n",
    "\n",
    "\n",
    "for i, row in enumerate(training_predictions):\n",
    "    video_id = re.sub('_.*', '', training_frames[i])\n",
    "    frame_id = re.sub('(.*_frame_)|(_tool.*)', '', training_frames[i])\n",
    "    tool_id = reverse_index_to_class[int(re.sub('.*_tool_', '', training_frames[i]))]\n",
    "    \n",
    "    if 'video_id' not in prediction_output_data:\n",
    "        for col in ['source', 'video_id', 'frame_id', 'tool_id'] + bb_col_names:\n",
    "            prediction_output_data[col] = []\n",
    "    \n",
    "    # Now go through each bounding box and add it\n",
    "    for bbi in range(prediction_params['n_objects']):\n",
    "        bb = training_predictions[i,bbi*5:(bbi+1)*5]\n",
    "                \n",
    "        if bb[0] < pred_score_threshold:\n",
    "            continue # Ignore this detected object\n",
    "            \n",
    "        prediction_output_data['video_id'].append(video_id)\n",
    "        prediction_output_data['frame_id'].append(frame_id)\n",
    "        prediction_output_data['tool_id'].append(tool_id)\n",
    "        prediction_output_data['source'].append('lstm')\n",
    "        \n",
    "        \n",
    "        for ci, col in enumerate(bb_col_names):\n",
    "            prediction_output_data[col].append(bb[ci])\n",
    "\n",
    "predicted_output_as_df = pd.DataFrame(prediction_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now save the outputs (video_id, tool, (score, bounding box)*n)\n",
    "yolo_output_data = {}\n",
    "video_id = 'S306T1'\n",
    "tools_of_interest = [0]\n",
    "\n",
    "for i, row in enumerate(prediction_training_generator.data[video_id]):\n",
    "    frame_id = str(i + prediction_training_generator.video_offsets[video_id]).zfill(8)\n",
    "    \n",
    "    # Instantiate our output df\n",
    "    if 'video_id' not in yolo_output_data:\n",
    "        for col in ['source', 'video_id', 'frame_id', 'tool_id'] + bb_col_names:\n",
    "            yolo_output_data[col] = []\n",
    "    \n",
    "    # Now go through each bounding box and add it\n",
    "    for oi in range(row[3][0]): # Total bound boxes detected\n",
    "        if row[1][0][oi] < pred_score_threshold:\n",
    "            continue\n",
    "        if int(row[2][0][oi]) not in tools_of_interest:\n",
    "            continue\n",
    "                \n",
    "        yolo_output_data['video_id'].append(video_id)\n",
    "        yolo_output_data['frame_id'].append(frame_id)\n",
    "        yolo_output_data['tool_id'].append(reverse_index_to_class[int(row[2][0][oi])])\n",
    "        yolo_output_data['score'].append(row[1][0][oi])\n",
    "        yolo_output_data['source'].append('yolo')\n",
    "    \n",
    "        for ci, col in enumerate(bb_col_names[1:]):\n",
    "            yolo_output_data[col].append(row[0][0][oi][ci])\n",
    "    \n",
    "yolo_output_data_as_df = pd.DataFrame(yolo_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_labels_data = {}\n",
    "\n",
    "if 'video_id' not in true_labels_data:\n",
    "    for col in ['source', 'video_id', 'frame_id', 'tool_id'] + bb_col_names:\n",
    "        true_labels_data[col] = []\n",
    "\n",
    "for f in prediction_training_generator.frame_ids:\n",
    "    frame_annotations = prediction_training_generator.get_frame_annotations(f, objects=tools_of_interest)\n",
    "    \n",
    "    video_id = re.sub('_.*', '', frame_annotations['path'])\n",
    "    frame_id = re.sub('.*frame_', '', frame_annotations['path'])\n",
    "    \n",
    "    for o in frame_annotations['objects']:\n",
    "        true_labels_data['video_id'].append(video_id)\n",
    "        true_labels_data['frame_id'].append(frame_id)\n",
    "        true_labels_data['tool_id'].append(o['class'])\n",
    "        true_labels_data['score'].append(1)\n",
    "        true_labels_data['source'].append('gt')\n",
    "        \n",
    "        for ci, col in enumerate(bb_col_names[1:]):\n",
    "            true_labels_data[col].append(o['coords'][ci])\n",
    "            \n",
    "true_labels_data_as_df = pd.DataFrame(true_labels_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_three_datasets = pd.concat([\n",
    "    yolo_output_data_as_df,\n",
    "    predicted_output_as_df,\n",
    "    true_labels_data_as_df\n",
    "])\n",
    "\n",
    "all_three_datasets.to_csv(\n",
    "    os.path.join(OUTPUTS_FROM_DETECTION_DIR, 'combined_call_boxes.csv'),\n",
    "    sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate labelled plots where for each image, we have the yolo, lstm, and gt bounding boxes drawn, in order to be able to visually compare a certain set of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames_only = [f for f in set([re.sub('_tool.*', '', f) for f in frames_predicting_one])]\n",
    "frames_only.sort()\n",
    "colors_map = {\n",
    "    'yolo': 'green',\n",
    "    'lstm': 'red',\n",
    "    'gt': 'blue'\n",
    "}\n",
    "\n",
    "out_spec_folder = 'yolo-lstm-gt'\n",
    "\n",
    "for f in frames_only:\n",
    "    matching_annotations = all_three_datasets[(all_three_datasets['video_id'] + '_frame_' + all_three_datasets['frame_id']) == f]\n",
    "    matching_annotations = matching_annotations.reset_index()\n",
    "    \n",
    "    image_path = os.path.join(IMAGES_DIR, f + '.jpeg')\n",
    "    img_array = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(img_array)\n",
    "    \n",
    "    # Draw the ground truth boxes\n",
    "    for ri in range(len(matching_annotations)):\n",
    "        coords = [\n",
    "            matching_annotations.at[ri, 'x1'],\n",
    "            matching_annotations.at[ri, 'y1'],\n",
    "            matching_annotations.at[ri, 'x2'],\n",
    "            matching_annotations.at[ri, 'y2']\n",
    "        ]\n",
    "        coords = [i  * img_array.shape[1] for i in coords]\n",
    "        \n",
    "        rect = Rectangle(\n",
    "            (coords[0], coords[2]), \n",
    "            coords[1] - coords[0], \n",
    "            coords[3] - coords[2], \n",
    "            linewidth=1,edgecolor=colors_map[matching_annotations.at[ri,'source']],facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect) \n",
    "        \n",
    "    plt.savefig(os.path.join(OUTPUTS_FROM_DETECTION_DIR, out_spec_folder, f + '.jpeg'))\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov3-tf2-cpu",
   "language": "python",
   "name": "yolov3-tf2-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
